{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "import nncf\n",
    "import openvino as ov\n",
    "import openvino_tokenizers as ovtk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models\n",
    "Download a sentiment analysis model and tokenizer from HuggingFace hub. And then, convert the sentiment analysis model and tokenizer into OpenVINO IR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321a820290754d4d828fe19187dfed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|   Num bits (N) | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+================+=============================+========================================+\n",
      "|              8 | 40% (34 / 77)               | 23% (30 / 73)                          |\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|              4 | 60% (43 / 77)               | 77% (43 / 73)                          |\n",
      "+----------------+-----------------------------+----------------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d2b58b1e774d6bb7b6931138ffeef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'MarieAngeA13/Sentiment-Analysis-BERT'\n",
    "\n",
    "# Convert a tokenizer\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "ov_tokenizer = ovtk.convert_tokenizer(hf_tokenizer)\n",
    "ov.save_model(ov_tokenizer, 'tokenizer.xml')\n",
    "\n",
    "# Convert and compress weight a sentiment analysis model\n",
    "config = AutoConfig.from_pretrained(model_id, output_hidden_states=True)    \n",
    "ov_model = OVModelForSequenceClassification.from_pretrained(model_id, config=config, export=True, compile=False, load_in_8bit=False)\n",
    "# Compress the model weights\n",
    "ov_model = ov_model.half()\n",
    "ov_compressed_model = nncf.compress_weights(ov_model.model, mode=nncf.CompressWeightsMode.INT4_ASYM, group_size=128, ratio=0.8)\n",
    "ov.save_model(ov_model.model, 'sentiment-analysis.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import openvino_tokenizers as ovtk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the sentiment analysis model and tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model: 'tokenizer'\n",
      "inputs[\n",
      "<ConstOutput: names[Parameter_1] shape[?] type: string>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[input_ids] shape[?,?] type: i64>,\n",
      "<ConstOutput: names[token_type_ids] shape[?,?] type: i64>,\n",
      "<ConstOutput: names[attention_mask] shape[?,?] type: i64>\n",
      "]> <Model: 'Model3'\n",
      "inputs[\n",
      "<ConstOutput: names[input_ids] shape[?,?] type: i64>,\n",
      "<ConstOutput: names[attention_mask] shape[?,?] type: i64>,\n",
      "<ConstOutput: names[token_type_ids] shape[?,?] type: i64>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[logits] shape[?,3] type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "ov_core = ov.Core()\n",
    "ov_tokenizer = ov_core.read_model('tokenizer.xml')\n",
    "ov_sentiment_model = ov_core.read_model('sentiment-analysis.xml')\n",
    "\n",
    "print(ov_tokenizer, ov_sentiment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the tokenizer model and sentiment analysis model\n",
    "Connect two models to make it a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CompiledModel:\n",
      "inputs[\n",
      "<ConstOutput: names[Parameter_1] shape[?] type: string>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[logits] shape[?,3] type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "ov_connected_model = ovtk.connect_models(ov_tokenizer, ov_sentiment_model)\n",
    "ov_compiled_connected_model = ov.compile_model(ov_connected_model, 'CPU')\n",
    "print(ov_compiled_connected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the integrated sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ConstOutput: names[logits] shape[?,3] type: f32>: array([[ 2.8169205, -0.9881832, -2.1669624]], dtype=float32)}\n",
      "Negative, Neutral, Positive\n",
      "[0.97171885 0.02162744 0.00665377]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=0)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "input_sentence = \"I lost my wallet. I'm so sad.\"\n",
    "res = ov_compiled_connected_model.infer_new_request([input_sentence])\n",
    "print(res)\n",
    "res = softmax(res['logits'].flatten())\n",
    "print('Negative, Neutral, Positive')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
