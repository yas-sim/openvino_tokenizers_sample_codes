{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from openvino_tokenizers import convert_tokenizer, connect_models\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the TinyLlama-1.1B-Chat tokenizer from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "model_vendor, model_name = model_id.split('/')\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "hf_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the downloaded tokenizer into OpenVINO IR models using 'OpenVINO Tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Model: 'tokenizer'\n",
      "inputs[\n",
      "<ConstOutput: names[string_input] shape[?] type: string>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[input_ids] shape[?,?] type: i64>,\n",
      "<ConstOutput: names[attention_mask] shape[?,?] type: i64>\n",
      "]> <Model: 'detokenizer'\n",
      "inputs[\n",
      "<ConstOutput: names[Parameter_22] shape[?,?] type: i64>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[string_output] shape[?] type: string>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "ov_tokenizer, ov_detokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=True)\n",
    "print(ov_tokenizer, ov_detokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the converted tokenizer and de-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ConstOutput: names[input_ids] shape[?,?] type: i64>: array([[    1,  4673, 29963,  1177, 29949,   338,   385,  1722, 29899,\n",
      "         4993,  5780,  7354,   363,  5994,  5281,   322,  7246,   292,\n",
      "         6483,  6509,  4733,   515,  9570,   304,  7636, 29889]],\n",
      "      dtype=int64), <ConstOutput: names[attention_mask] shape[?,?] type: i64>: array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]], dtype=int64)}\n",
      "{<ConstOutput: names[string_output] shape[?] type: string>: array(['OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge.'],\n",
      "      dtype='<U104')}\n",
      "['OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge.']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compile the OV Tokenizer models for a specific device (CPU)\n",
    "compiled_tokenizer   = ov.compile_model(ov_tokenizer, 'CPU')\n",
    "compiled_detokenizer = ov.compile_model(ov_detokenizer, 'CPU')\n",
    "\n",
    "# Tokenize\n",
    "input_string = 'OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge.'\n",
    "token_ids = compiled_tokenizer.infer_new_request([input_string])\n",
    "print(token_ids)\n",
    "\n",
    "# Detokenize\n",
    "detokenize_result = compiled_detokenizer.infer_new_request(token_ids['input_ids'])\n",
    "print(detokenize_result)\n",
    "print(detokenize_result['string_output'])\n",
    "\n",
    "print(input_string == detokenize_result['string_output'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the IR models for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.save_model(ov_tokenizer, 'ov_tokenizer.xml')\n",
    "ov.save_model(ov_detokenizer, 'ov_detokenizer.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
